

== Informative Implementation Guidance

=== Clarification: Publisher Considerations

The CDB Standard does not provide guidelines regarding its implementation within specific vendor SE toolsets and vendor simulation architectures. This clearly falls outside of the scope of the CDB Standard. The CDB Standard is focused a physical and logical storage structure and guidance on storage formats for information (i.e., it merely describes data) for use in simulation.

The CDB model lends itself to a real-time implementation within simulation architectures. This capability requires that the vendor’s client-device be adapted with a Run-Time publishing (RTP) software function which transforms the CDB structured data into the client-device’s internal legacy/proprietary format. This is a new concept for the simulation industry and consequently there is considerable confusion regarding the implementation of Off-line and Run-Time Publishers (RTPs). While much of the attention has focused on RTPs, a similar set of considerations apply to the implementation of an off-line CDB capability (CDB is used as a Refined Source Data Repository). In this latter case, the capability requires that the vendor develop an off-line CDB import function which ingests the CDB into their Synthetic Environment Creation toolset. Once imported, the vendor toolset could produce the vendor’s proprietary data format through an off-line compilation function.

By definition, the function of an RTP is to bridge the “gap” (or adapt) between CDB data schema and the client-device’s internal (proprietary) data schema. Since this gap is unknown, it is impossible in this addendum to provide hard-and-fast rules and detailed estimates for the implementation of an RTP (or a CDB import function).

Note that there are many alternatives open to a vendor when considering the level of compliancy he wishes to achieve. The level-of-effort is essentially a function of the level of compliancy the vendor wishes to achieve, and the size of the intrinsic “gap” between the CDB data schema and his device’s internal schema.

Nonetheless, this section highlights aspects of the CDB that are particularly significant when considering such implementations. These aspects dominate the level-of-effort required to achieve ideal CDB compliancy.

The CDB Standard limits itself to a description of a data schema for Synthetic Environmental information (i.e. it merely describes data) for use in simulation. The CDB Best Practices provide rigorous guidance of the semantic meaning for each dataset, each attribute and establishes the structure/organization of that data as a schema comprised of a folder hierarchy and files with internal (industry-standard) formats. This ensures that the all CDB data is understood, interpreted and efficiently accessed in the same way by each client-device. The CDB standard does not include detailed guidelines regarding off-line database compliers or runtime publisher implementations, since this would be tantamount to dictating internal vendor formats which are by their very nature proprietary.

The CDB Standard DOES NOT specify:

* The implementation details of an off-line CDB import function that can then be used to compile the imported Synthetic Environmental data into one or more types of proprietary runtime databases (only the client-device vendor has this knowledge and control);

* The implementation details or algorithms of runtime publishers attached to specific client-device (only the client-device vendor has this knowledge and control); or

* The implementation details or algorithms of client-devices that use CDB data (only the client-device vendor has this knowledge and control).


While the CDB standard does not govern the actual implementation of client-devices, it is expected that the CDB standard will have a “unifying” effect on the implementation of each vendor’s client-device by virtue of the fact that they will share the exact same Synthetic Environmental data. It is expected that side-by-side comparisons will be easier to undertake due to the fact that devices will run off the exact same runtime data. Prior to the advent of the CDB standard, side-by-side comparisons were considerably more difficult to undertake due to the fact the entire SE creation chain starting from raw source was implicated in such evaluations.

If we set aside legacy considerations, the simplest approach to adopting the CDB would require that client-devices ingest the CDB natively, i.e., client-devices would handle all of the CDB data schema/semantics without any off-line or run-time intermediary.

In practice however, most vendors have extensive legacy SE assets and cannot afford to obsolesce these. As a result, most client-devices must continue to support their own proprietary legacy runtime databases. Given these considerations, two solutions are possible.

1.  No change to the Client-device: In this approach, vendors have chosen to achieve an off-line CDB capability (CDB is used as a Refined Source Data Repository). This capability requires that the vendor develops an off-line CDB import function which ingests the CDB into his Synthetic Environment Creation toolset; once imported, the toolset produces (as always) the vendor’s proprietary data format through an off-line compilation function.
2.  Level-of-Effort of a Publisher Implementation

The following discussion attempts to qualify the level-of-effort to achieve CDB compliancy. The discussion applies equally to both paradigms, i.e., the CDB Runtime Publishing paradigm and the CDB import-then-compile paradigm.

In the case where a client-device already supports most of data schema and semantics concepts of the CDB, then the RTP (or import-then-compile) software is proportionally less complex. For instance, if an IG already supports the concepts of tiles, of levels-of-detail, of layers and understands the concepts of datasets such as terrain texture, gridded terrain elevation, gridded terrain materials, etc. then there is a modest amount of work to be performed by an RTP.

NOTE: The level-of-effort in adopting the CDB model is proportional to the difference between the CDB data schema and client-device’s internal proprietary data schema.


Clearly, the algorithmic complexity of an RTP and the computational load imposed on the RTP is directly proportional to the above-mentioned “gap”. The larger the “gap”, the more expensive a RTP is to develop and the more computational resources need to be allocated to implement it. Conversely, with a smaller “gap”, the RTP development is straightforward and relatively few computational resources need to be allocated to this function.

In order to assess the level-of-effort to adopt the CDB model, the vendor must first evaluate the similarity of data schemas between the CDB and his client-device, in particular, the vendor must assess whether they espouse the following fundamental CDB concepts:

* Independent Tiles (used for paging of all data)
* Independent Levels-of-Detail (for all data)
* Independent Layers (Dataset Layering)
* Following dataset concepts and semantics:
** Semantic understanding of all CDB layers
** Geo-gridded data layers consisting of terrain altimetry, terrain texture, terrain materials/mixtures
** Geo-defined vector features (points, lineals, areals)
*** With/without modeled 3D representations
*** Feature attribution
** 3D Modeled representation of features (using a data schema similar to or equivalent to OpenFlight)
*** Instanced geotypical models
*** Instanced model geometry
*** Instanced model texture
*** Non-instanced geospecific models
** Conforming of features to terrain skin (e.g. height conforming)
** Topological networks
** JPEG-2K compression
** Generation of device-specific NVG/FLIR rendering parameters for light-points and materials

In the case where a client-device does not intrinsically support one or more of the above-mentioned CDB concepts, the RTP must perform SE conversions that will likely fall beyond those of mere format/structure manipulations. Such conversions may affect the precision of the data, its semantic meaning, etc. and thus can compromise certain aspects of runtime correlation.

The CDB data model favors modern up-to-date implementations of client-devices. In effect, the level-of-effort to develop an RTP for an obsolete legacy device is likely to be greater than for a modern device. This is because early approaches in digital computer based flight simulation were more severely constrained by severe hardware, software and data source limitations. Consequently, simulation engineers made important compromises between a subsystem’s targeted fidelity and its level of generality, scalability, abstraction, and correlation with other simulator client-devices. In many cases, engineers reverted to complex support data structures (generated off-line) in order to reduce the computational load at runtime.

A classic example of this was the use of Binary Separation Planes (BSPs) data structures footnote:[Such BSP data structures where required by most IG vendors prior to ~1995 due to the fact that the IGs did not have sub-pixel level Z-buffer capability.] which were required prior to the widespread adoption of Z-buffers by the IG vendors. The CDB standard does not make provisions for this and as such, the RTP for legacy BSP-based IG devices would be burdened with the rather difficult task to generate BSPs in real-time.

Given their tremendous benefit, the concepts of paging (e.g. tiles) and levels-of-details have steadily been adopted by simulation vendors over the past 15-20 years and have been applied to most datasets, notably terrain and imagery datasets. (See Appendices G and F of the Volume 2: OGC CDB Core Model and Physical Structure Annexes for a rationale for Tiles and Levels-of-detail). As a result, it is not expected that the CDB tiles and LOD concepts will be a problem for most vendors. Note however that CDB applies these two concepts to ALL dataset layers including vector features and 3D models.

==== Client-Devices

Each client-device is matched either to an off-line compiler or to a runtime publisher. In the runtime case, the runtime publisher transforms this data into the client-device’s legacy native data format and structures the CDB synthetic environment data as it is paged-in by its client-device. Regardless of its use as an offline or online repository, implementing the CDB standard eliminates all client-format dependencies. Alternately, the client-device may be designed / modified to be CDB-native, in which case a separate runtime publisher is not required. Note that the CDB standard makes use of data types commonly available in standard computer platforms (floats, integers, etc.). While it would be theoretically possible to cater to a client-device that does not support the “atomic” data types, it would unduly load the attached online publisher. As a result, it is recommended that all client-devices provide hardware support for the CDB specified atomic data types.

Since it is the client-devices that initiate access to the CDB conformant data store, they must each be theoretically “aware” of at least the geodetic earth reference model footnote:[http://onlinelibrary.wiley.com/doi/10.1029/EO062i007p00065/abstract]. Otherwise, the contents and the structure of the data store instance can be completely abstracted from the client-device.

==== Typical Functions Performed by a Publisher Implementation

The following discussion provides a typical list of software functions that must be developed in order to achieve CDB compliancy. The discussion applies equally to both paradigms, i.e. the CDB Runtime Publishing paradigm and the CDB import-then-compile paradigm.

Virtually all simulation client-devices in existence today natively ingest their own proprietary native runtime formats. In order to ingest CDB structured data directly, vendors must adapt the device’s software to natively ingest the currently defined CDB formats footnote:[The number of specified formats will be expanded in future versions of the CDB standard.] (e.g. TIFF, Shape, OpenFlight, etc.) or alternately, they can insert a runtime publisher function that transforms the CDB data formats into legacy client device’s native runtime format. The runtime publishing process is performed when the CDB is paged-in from the CDB storage device.

The runtime publishers are nothing more than well-optimized offline publishers capable of responding to the on-demand compilation of datasets as they are being paged-in by the respective client devices. The function of a runtime publisher is no different than that of a conventional offline database publisher, i.e., it…

a.  transforms the assembled data store so that it satisfies the client-device’s internal data structure and format
b.  transforms the assembled data store so that it satisfies the client-device’s internal naming conventions
c.  transforms the assembled data store so that it satisfies the client-device’s number precision and number representation
d.  transforms the assembled data store into parameters compatible with the client device’s internal algorithms (typically light parameters, FLIR/NVG parameters, etc.
e.  transforms the assembled data store so that it satisfies the client-device’s data fidelity requirements
f.  transforms the assembled data store so that it satisfies the client-device’s performance and internal memory limitations
g.  transforms the assembled data store so that it satisfies the client-device’s level of-detail representation requirements.

*Ideally, the scope of an RTP should be purely limited to manipulations of data format and data structure and internal naming conventions (items a-g above). Under such circumstances, it is possible to achieve perfect runtime correlation between client-devices.*

==== Publisher Implementation Recommendations

The use of the CDB data schema “as-is” by a client-device achieves all of the benefits stated in sections 1.4 and 1.5 of the CDB Standard, namely:

a. Improved SE generation timeline and deployment
b. Interoperable simulation-ready SE
c. Improved client-device robustness/determinism
d. Increase SE longevity
e. Reduced SE storage infrastructure cost
f. Platform independence and scalability
g. SE scalability and adaptability

In the case where a client-device does not adhere to one or more of the above-mentioned “fundamental CDB concepts”, fewer of the CDB benefits will be realizable.

For instance, a client-device incapable of dealing with levels-of-detail will not have the same level SE scalability (a benefit explained in section 1.4.7 of the CDB Standard) as one that fully espouses that concept. While the latter may be acceptable, it is clearly a less-compliant and an inferior implementation of the CDB than the former.

Changes to the modeled representation of features are generally not advisable since it invariably affects the accuracy of the modeled representation. Most image generators in use today can ingest a (one-for-one correspondence) the CDB modeled polygonal representation of 3D features. However, in the case of terrain, there are two dominant approaches in industry, either a regular grid with LODs or alternately, the Terrain Irregular Network (TIN) mesh. The CDB Standard has opted for the former given its greater scalability, determinism and compatibility with tiling schemes. Clearly, implementations where such conversions are not necessary are advantaged and provide more of the above-mentioned CDB benefits.

Furthermore, the CDB is designed to provide both the semantic (e.g. vector data/attribution) and the modeled representation of features. Since the CDB Standard and associated Best Practices provides both, it is not advisable to ignore or replace the modeled representation (if provided) nor is it advisable to synthesize a non-CDB modeled representation if none was supplied within the CDB. While the CDB Standard does not forbid vendors to interpret CDB feature data for the purpose of procedurally synthesizing more detailed feature data or synthesizing modeled data from the feature data, _this practice is not recommended as this would severely compromise correlation and inter-operability_. In the context of correlated synthetic environments, such approaches are viable if and only if all client-devices in a federation are equipped with the exact same procedural algorithms. Currently, this is not possible because there are no industry-standard, open-source procedural algorithms endorsed by all simulation vendors.

In the case of the CDB Runtime Publishing paradigm and the CDB import-then-compile paradigm, it is not advisable to ignore or replace the modeled representation (if provided) nor is it advisable to synthesize a non-CDB modeled representation if none was supplied within the CDB.

=== Use of a CDB conformant data store as an Off-line Repository

Figure 1-1: Use of a CDB conformant data store as an Off-line Repository, illustrates the deployment process of a CDB conformant database when it is used solely as an off-line Master repository. This approach follows the SE deployment paradigm commonly used today within the simulation community. The use of a CDB conformant data store as an off-line environmental data repository offers immediate benefits, namely…

* SE Standardization through a public, open, fully-documented schema that is already supported by several SE authoring tools.
* SE Plug-and-Play Portability and Interoperability across various vendor SE authoring toolsets
* SE Correlation through the elimination of source correlation errors through normalization of all data sets (a single representation for each dataset)
* SE Re-use by eliminating dependencies that are specific to the simulation application, the Data store Generation tool suite, the simulation program, the technology
* SE Scalability which results in near-infinite SE addressability, spatial resolution and content density in each of the SE datasets.
* 3D Model Library Management through built-in provisions for the cataloging of models
* SE Versioning Mechanism allowing instant access to prior versions and simplified configuration management
* Cooperative SE Workflow through an internal SE structure which favors teamwork. The SE workflow can be allocated by specialty (e.g., altimetry, satellite imagery, vector data) or by geographic footprint.
* Straightforward SE Archival and Recovery

Note that the use of the use of CDB conformant data store as an offline repository does not impose any change to the simulation training equipment (i.e., no modifications to client-devices are required footnote:[Or alternately, runtime publishers need not be developed for client-devices]). However, the deployment of the synthetic environment is similar to the conventional approaches used in industry requiring the time-consuming, storage-intensive, off-line compilation of proprietary runtime databases to each client-device. Furthermore, the computing demands on the data store generation facility are significantly greater because the entire data store must be published off-line for each client-device before it can be deployed. These costs rapidly escalate with the complexity and size of the synthetic environment, the number of supported client-devices and the number of supported training facilities. For complex data stores, these costs can far outweigh the costs of the runtime publishers attached to each simulator client-device.

image::images/image1.png[image,width=644,height=334]

[#img_UseofCDBConformantDatabaseasanofflineDatabaseRepository,reftext='Figure 1-1']
*Figure 1-1. Use of CDB Conformant Database as an off-line Database Repository*


In most modern SE tool suites in-use today, the Data Preparation step shown in Figure 1-2: SE Workflow with a CDB structured data store as an Off-line Repository consists of many sub-steps usually applied in sequence to each of the datasets (aka layers) of the SE. In effect, this aspect of the modeler’s responsibilities is virtually identical to that of a GIS footnote:[__G__eographic __I__nformation __S__ystems] specialist. As a result, many of the simulation equipment vendors offer SE authoring tools that integrate best-of-breed COTS footnote:[__C__ommercial-__O__ff-__T__he-__S__helf] GIS tools into their respective tool suites. The steps include the following.

* _Format conversion:_ raw source data is provided to modelers in literally hundreds of formats. Early on in the SE generation process, modelers typically settle on a single format per SE layer (e.g., terrain altimetry, imagery, attribution)
* _Error handling:_ raw source often contains errors or anomalies that, if left undetected, corrupt and propagate through the entire SE data preparation pipeline. As a minimum, these errors must be detected early on in the process. More advanced tools can correct many of these automatically, particularly if there is some redundancy across the layers of data.
* _Data geo-referencing:_ this is the process of assigning a unique location (latitude, longitude and elevation) to each piece of raw data entering the SE pipeline.
* _Data Registration:_ each dataset is manipulated so that it coincides with information contained in the other datasets. These manipulations include projections, coordinate conversions, ortho-rectification, correction for lens distortions, etc. For images, this process is also known as rectification.
* _Data Harmonization:_ the raw data of a dataset varies over a geographic extent if it was obtained under different conditions, such as from two or more sensors with differing spectral sensitivity characteristics, resolution, in different seasons, under different conditions of weather, illumination, vegetation and human activity. The modeler must factor for these variations when selecting and assembling the datasets into a self-coherent SE.

image::images/image2.jpeg[untitled1,width=602,height=408]

[#img_SEWorkflowwithCDBasanoff-lineRepository,reftext='Figure 1-2']
*Figure 1-2. SE Workflow with CDB as an off-line Repository*


The effort expended during the Data Preparation and Modeling step is mostly independent of the targeted simulation devices and the targeted applications. Consequently, the results of the data preparation step can be stored into a Refined Source Data Store (RSDS) and then re-targeted at modest cost to one or more simulation devices.

The standardization of simulation data stores can greatly enhance their portability and reusability. The CDB Standard and associated OGC Best Practices offers a standardized means to capture the effort expended during the Data Preparation and Modeling step. In effect, a CDB structured database becomes a master repository where refined source can be “accumulated” and managed under configuration control.

While standardization of format/structure is essential to achieve high portability, interoperability and reuse, the SE content must be ideally developed so that its content is truly independent of the training application. Therefore, we strongly recommend that the SE content of the CDB structured repository be developed to be independent of the training application.

Historically, SEs were developed for a single, targeted simulation application (e.g., tactical fighter, civil and air transport, rotary wing, or ground/urban warfare). In effect, the intended training application played an important role in determining the RSDB content because SE developers were constrained by the capabilities of the authoring tools and of the targeted simulation device. Unfortunately, this tailoring of SE was performed too early during the SE workflow and severely limited the applicability and re-use of the SE. Application tailoring can require either data intensification footnote:[Data Intensification is the process of augmenting or deriving added detail from the information found in the raw data. For instance, intensification can be used to augment flattened terrain imagery with 3D cultural detail relief. A typical example of this consisting in populating forested areas found in the terrain imagery with individual three-dimensional trees.] or data decimation footnote:[Data Decimation is the process of removing or simplifying the informational content found in the raw data. For instance, decimation can be used to transform individually modeled buildings into simplified city blocks or to reduce the resolution of terrain imagery. Data decimation is usually undertaken to ensure that the SE falls within the capabilities of the targeted simulator system.] .

Once the SE developer has completed his work in creating the various data layers of the RFDS, he must offline publish (aka “compile”) the SE into one or more device-specific data publishing steps. As we will discuss in section 6.4, Use of CDB structured data store as a Combined Off-line and run-time data store Repository, the device-specific off-line compilation step can be entirely omitted if the targeted training equipment is CDB-compliant.

While an off-line publishing approach does not offer all of the benefits described in this section, it nonetheless provides an easy, low-effort, migration path to CDB. Any equipment vendor can easily publish the data into their proprietary runtime format. Firstly, the publishing process is facilitated by the fact that the CDB standard provides guidance on how to use industry standard formats. However, the CDB model goes much further in that it specifies how to use these formats in a global, standardized data model suited to high-end real-time simulations. This greatly facilitates the work of SE developers. Thus, the CDB model provides a far simpler and straightforward means of interchanging refined source data.

=== Use of a CDB conformant data store as a Combined Off-line and Run-time Data store Repository

A data store conforming to this CDB standard can be both used an offline repository for authoring tools or as an on-line (or runtime) repository for simulators. When used as a runtime repository, a CDB conformant data store offers plug-and-play interchangeability between simulators that conform to the CDB standard. Since a CDB conformant data store can be used directly by some or all of the simulator client-devices, it is considered a run-time environment data store.

In addition to the benefits outlined in section link:#h.44sinio[6.3], the use of the CDB conformant data store as a combined off-line and run-time repository offers many additional benefits.

* SE Plug-and-Play Portability and Interoperability across CDB-compliant simulators and simulator confederacies (be it tactical air, rotary, urban/ground, sea).
* Reduced Mission Rehearsal Timeline by eliminating SE generation steps (off-line publishing, database assembly and data automation
* Simplified Deployment, Configuration Control and Management of Training Facility SE Assets by eliminating the duplication of SE runtime DBs for each simulator and each client-device of each simulator.
* Single, centralized storage system for the SE runtime repository (can be extended to a web-enabled CDB)
* Seamless integration of 3D models to the simulator.
* Fair Fight/Runtime Content Correlation through the adjustment of runtime level-of-detail control limits at each client-device.


Figure 1-3: Use of CDB Model as an Off-line and On-line Data Store Repository, illustrates the CDB structure as an off-line Master data store repository for the tools and as an online Master data store repository for the training facilities. Note that the deployment of the synthetic environment to the training facilities involves a simple copy operation. The deployment of a CDB conformant data store is further simplified through an incremental versioning scheme. Since only the differences need be stored within the data store, new versions can be generated and deployed efficiently.

image::images/image3.png[image,width=634,height=396]

[#img_UseofCDBasanOfflineandOnlineDataStoreRepository,reftext='Figure 1-3']
*Figure 1-3. Use of CDB as an Off-line and On-line Data Store Repository*



The CDB standard associated Best Practices specify formats and conventions related to synthetic environments for use in simulation. However, many additional benefits can be garnered if a CDB structured data store is also used as an online data store repository. This is particularly true when one considers the effort expended in the deployment of the synthetic environment to the training and/or mission rehearsal facilities.

When used as an online data store repository, there is no need to store and maintain off-line published versions of the data store for each client-device (as illustrated in Figure 1-3). As a result, the storage and computing demands on the data store generation facility are significantly lowered. This is especially true of data store generation facilities whose mandate involves the generation of complex synthetic environments for use by several training facilities.

Figure 1-4: SE Workflow with CDB as Combined Off-line/Runtime Data Store Repository, illustrates the simplified database generation workflow resulting from a data store that is used as both an offline and a runtime SE repository.

image::images/image4.jpeg[untitled1,width=663,height=437]

[#img_SEWorkflowwithCDBasCombinedOfflineRuntimeDataStoreRepository,reftext='Figure 1-4']
*Figure 1-4. SE Workflow with CDB as Combined Off-line/Runtime Data Store Repository*


This approach permits the CDB representation of the synthetic environment to be “dissociated” from the resolution, fidelity, precision, structure and format imposed by the internals of client-devices. Compliancy to the CDB standard can be achieved either by modification of the client-device internal software to make it CDB-native or by inserting a runtime publishing process that transforms the CDB structured data into the client-device’s legacy native runtime format. In the later case, this process is done in real-time, on a demand-basis, as the simulator “flies” within the synthetic environment. Note that since the simulated own ship footnote:[Own ship is the object you are on. Target ship is the object you are watching.] moves at speeds that are bounded by the capabilities of the simulated vehicle, it is not necessary to instantly publish the entire synthetic environment before undertaking a training exercise; the runtime publishers need only respond to the demands of the client-devices. When the simulated own-ship’s position is static, runtime publishers go idle. As the own ship starts advancing, client-devices start demanding for new regions, and runtime publishers resume the publishing process. Publishing workload peaks at high-speed over highly resolved areas of the synthetic environment.

Note that virtually all simulation client-devices in existence today natively ingest proprietary native runtime formats. As a result, a runtime publisher is required to transform the CDB structured data into legacy client device’s native runtime format. The runtime publishing process is performed when the CDB conformant database is paged-in from the CDB storage device. Volume 7, OGC CDB Data Model Guidance provides a set of guidelines regarding the implementation of Runtime Publishers.

=== Example Implementation of a CDB Structured Data Store on a Simulator footnote:[Legacy simulator client-devices can be readily retrofitted for compatibility with the CDB Standard by inserting a runtime publisher in their SE paging pipeline.]

This section illustrates a possible implementation architecture of the CDB Standard on a flight simulator. The standard does not mandate particular simulator architecture or the use of specific computer platforms. The selected implementation varies with the required level of fidelity and performance of the simulator and its client-devices.

As shown in Figure 1-5: _Typical CDB Implementation on a Suite of Simulators_, a typical implementation of a CDB compliant system consists of the following main components.

1.  Data Store Generation Facility (DBGF) and CDB Master Store: A geographically co-located group of workstation(s), computer platforms, input devices (digitizing tablets, etc.), output devices (stereo viewers, etc.), modeling software, visualization software, database server, off-line publishing software and any other associated software and hardware used for the development/modification of the data store. The CDB Master Store consists of a mass storage system (typically a storage array) and its associated network. It is connected to a dedicated DBGF Server.
2.  Update Manager (UM): The Update Manager software consists of both client and server software. The Update Manager Server (UMS) software is located at the DBGF. It manages the data store updates (versions) and runs in the same platform as the DBGF Server. The Update Manager Client (UMC) software is located at the Simulator Facility and runs on the Update Manager Platform shown in Figure 1-5: _Typical CDB Implementation on a Suite of Simulators_. The UMC communicates with the UMS to transfer the data store (partial or complete copy) and its updates.
3.  Simulator Facility CDB Data Store Repository: The simulator repository consists of a mass storage system (typically a storage array) and its associated network infrastructure. It is connected to the UMC (primarily for update purposes) and the servers (for simulator client-device runtime access).
4.  CDB servers: An optional footnote:[Optionally needed for a large-scale CDB repository whose storage system is based on a Storage Area Network (SAN).] gateway to mass storage and applicable infrastructure. The CDB servers access, filter and distribute data in response to requests from the simulator runtime publishers.
5.  Runtime publishers: A term used to describe the computer platforms, and the software that translates and optimizes, at runtime, CDB synthetic environment data store to a client-device specific legacy runtime format. Data is pulled from the CDB server and in turn published in response to requests from its attached simulator client-device.
6.  Simulator client-devices: Are simulation subsystems (IGs, radar, weather server, Computer Generated Forces (CGF) terrain server, etc.) that require a complete or partial synthetic representation of the world. CDB runtime clients may require a CDB runtime publisher to convert the CDB into a form they can directly input.

image::images/image5.png[image,width=669,height=399]

[#img_TypicalCDBImplementationonaSuiteofSimulators,reftext='Figure 1-5']
*Figure 1-5. Typical CDB Implementation on a Suite of Simulators*


==== Data Store Generation Facility (DBGF)

The DBGF is used for the purpose of CDB structured database creation and updates. Each workstation is equipped with one or more specialized tools. The tool suite provides the means to generate and manipulate the synthetic environment.

==== Database Generation Flow

The CDB Model considerably simplifies the data store generation process, particularly all aspects of data store generation that deal with data store layering, formatting, structure and level-of-detail.


image::images/image6.png[image,width=614,height=371]

[#img_TypicalDBGenerationCDBUsedasaDBRepository,reftext='Figure 1-6']
*Figure 1-6. Typical DB Generation - CDB Used as a DB Repository*


Figure 1-6: _Typical DB Generation - CDB Used as DB Repository_ and Figure 1-7: _Typical DB Generation Flow - CDB Used as DB & Sim Repository_ illustrate a typical database generation workflow with the database used as a DB workstation repository and the database used as a Repository for the DB workstation and the simulator. Both approaches share the same steps, namely…

1.  _Source data collection and preparation:_ This step usually involves the loading of raw (usually) uncorrected data and the conversion to formats native to the data store toolset.
2.  _Source data preparation:_ This step usually involves the detection/correction of errors, the harmonization of the data and the correction of errors. In this context, errors signify all instances where the data fails to meet prescribed criteria. For instance, errors can be as straightforward as corrupted digital data. More subtle forms of errors could be textures that fail to meet various brightness, contrast, chrominance, and distortion criteria. Harmonizing data requires that data sources be coherent with each other. An example of non-harmonized dataset is a terrain imagery mosaic built from pictures taken in different seasons, with different illumination conditions, with/without clouds, etc.
3.  _3D modeling of features:_ This step involves the creation of 3D representations for culture features (buildings, trees, vehicles, etc.), the creation and mapping of texture patterns/imagery to the geometrical representation, the generation of the model LOD, and the generation of appropriate attribution data so that the simulator can control the model and have it respond to the simulated environment.
4.  _Data Store automation:_ Modern data processing and validation tools offer an increasing level-of-automation to the modelers, thereby improving the DB generation timeline (for example, a forest tool that controls the placement of individual trees correlated to the underlying terrain imagery). Over the past few years, tool vendors have introduced a broad set of tools aimed at eliminating highly repetitive modeling tasks. This includes tools for runway generation (including the positioning of stripes, lights, signs, markings, etc.), road/railroad generation, cultural feature extraction from stereo pairs, cultural feature footprint extraction from image classification processes, terrain grid generation from stereo pairs, terrain surface material classification, etc.

image::images/image7.png[image,width=636,height=503]

[#img_TypicalDBGenerationFlowCDBUsedasDBandSimRepository,reftext='Figure 1-7']
*Figure 1-7. Typical DB Generation Flow - CDB Used as DB and Sim Repository*


The result of the above steps yields a group of independent, layered and correlated datasets, (i.e., datasets that are geographically aligned in latitude/longitude (but not always elevation)), all sharing compatible projections, with all of the necessary attribution.

Out of the many steps typically required by the off-line compilation, the CDB structured data store only requires that levels-of-detail be generated for the terrain elevation, raster imagery, and the grouping of cultural features. These improvements are expected to yield important savings in man hours, machine hours and storage when compared to the non-CDB approach.

==== Update Manager

The creation of the CDB structured data store and subsequent updates are performed at the DBGF. The Update Manager (UM) keeps track of these updates and synchronizes the Simulator CDB Repository to the DBGF. The CDB Standard permits flexible and efficient access of the data store and does so with different levels of granularity. Thus, it is possible to perform modifications to the database on a complete tile, or on individual datasets of a tile. This permits rapid deployment of the data store, a feature that is particularly valuable for mission planning and rehearsal. With few exceptions footnote:[The only exceptions to this CDB principle are the MinElevation, MaxElevation datasets which are slaved to the Terrain Elevation dataset and the MaxCulture dataset which is slaved to the GSFeature/GTFeature dataset.], there is no interdependency between datasets and it is possible to modify a dataset (such as the terrain imagery) without reprocessing the complete tile; only the modified dataset requires re-processing. The CDB Standard supports the concurrent creation/modification of the data store with its deployment. Once a tile, a feature set, or a dataset has been processed, it may be transferred to the simulator facility concurrently with other work performed at the DBGF.

Updates to the simulator CDB structured repository are performed by the UM. The simulator CDB repository is configured to provide storage for a (partial or complete) copy of the Data Store Generation Facility (DBGF) master store. The Update Manager transfers the data store and its updates by area of interest, allowing for partial updates or even complete copies of the database. The Update Manager (UM) simulator CDB structured repository is used by one or more co-located simulators to retrieve the data store in real-time.

Additionally, the UM manages the facility’s release of the data store. It maintains versioning information as supplied by the DBGF. Based upon this information, it is possible to request or approve data updates to the facility from the UM.

==== CDB Servers

When a CDB structured data store is used as an on-line (or runtime) repository, a set of CDB servers (i.e., the server complex) is required in order to fetch data in real-time from the simulator CDB structured repository. Each of the CDB servers responds to the requests made by the simulator client-device runtime publishers.

==== Runtime Publishers

When the CDB structured data store is used as an on-line (or runtime) repository, a set of runtime publishers are required in order to transform the CDB data into legacy client-devices (simulator subsystems) internal format footnote:[Alternately, client-devices can be designed / modified to natively handle the CDB’s data model, thereby obviating the need for a separate runtime publishing step.]. The runtime publishers provide a key role in further enhancing overall algorithmic correlation within and across simulators. Each publisher communicates to the CDB data store server complex and the attached simulator client-device as follows.

1.  Receive update requests for synthetic environment data from their respective simulator client-devices.
2.  Relays the update request to the CDB server complex.
3.  Once the update request is acknowledged and the data retrieved by the CDB server complex, the runtime publisher pulls data from the CDB server complex and converts and formats this data into a form directly usable by the simulator client-device. This processing is accomplished in real-time.
4.  Transfers the converted data to the simulator client-device.

==== Simulator Client-devices

The sections below provide a short description of the client-devices found on a typical simulator and the global types of information required from the CDB.

===== Visual Subsystems

Typical visual subsystems compute and display in real-time, 3D true perspective scenes depicting rehearsal and training environments for OTW, IR, simulated Night Vision Goggles (NVG), and 3D stealth IG viewing purposes.

===== Out-The-Window Image Generator (OTW IG)

The IG portion of the visual system provides a wide range of features designed to replicate real-world environments. High density and high complexity 3D models can be superimposed onto high-resolution terrain altimetry and raster imagery. Scene complexity with proper object detail and occulting provide critical speed, height and distance cueing. Special effects are implemented throughout the data store to enhance the crew’s experience and overall scene integrity. Typical IGs optimize the density, distribution and information content of visual features in the scene(s) for all conditions of operations.

The visual subsystem uses time invariant information held in the CDB such as:

1. Terrain altimetry and raster imagery data
2. Cultural feature data
3. Light point data
4. Airport data
5. Material attribution data

===== Infrared IG

Included in the CDB Standard and associated Best Practices is the material attribution used by a typical physics-based Infrared Sensor Synthetic environment Model. This model computes, in real-time, the amount of radiated and propagated energy within the simulated thermal bands.

A typical thermal model takes into account the following material properties:

1. Solar absorbance
2. Surface emissivity: This coefficient reflects the degree of IR radiation emitted by the surface.
3. Thermal conductivity
4. Thermal inertia: This coefficient describes the material ability to gain/lose its heat to a still-air environment.


===== Night Vision Goggles Image Generation

Included in the coding is the material attribution (exclusive of any properties) used by NVG simulation models.

===== Ownship-Centric Mission Functions

Visual subsystems typically provide a set of ownship-centric Mission Functions (MIF) for use in determining…

1.  The Height Above Terrain (HAT), Height Above Culture (HAC), and Height Above Ocean (HAO). This function may report the material type of the texel or the polygon, and the normal of the surface immediately beneath the point.
2.  Own-ship Collision Detection (CD) with terrain, 3D culture and moving models. This may include long thin objects such as power lines.
3.  Line Of Sight (LOS) and Laser Ranging Function (LRF) originating from the ownship. This function may return the range, the material type and the normal of the nearest encountered element in the database. The maximum length of a requested vector is typically limited to the paged-in database.

The mission functions provided by an IG base their computations on data that has LOD representations equivalent to those used by OTW IGs. Since the visual subsystem scene management mechanisms are essentially slaved to the own-ship’s position, the terrain accuracy (e.g., its LOD), the cultural density/LOD and the texture resolution decrease with distance from the own-ship. As a result, the IG-based mission functions computations are best suited for own-ship functions. In cases where the data store needs to be interrogated randomly anywhere in the gaming area, simulator client-devices such as Computer Generated Forces (via a terrain server) are best suited because their architecture is not own-ship-centric.

===== Computer Generated Forces (CGF)

CGF provides a synthetic tactical environment for simulation-based training. A CGF application simulates behaviors and offers interactions between different entities within the simulation. It models dynamics, behavior doctrines, weather conditions, communications, intelligence, weapons and sensor interactions, as well as terrain interactions. CGF offers modeling of physics-based models in a real-time natural and electronic warfare environment for air, land and sea simulations.

Typically, CGF is able to create a realistic simulated multi-threat, time-stressed environment comprising items such as:

1. Friendly, enemy and neutral entities operating within the gaming area
2. Interaction with weather conditions currently in the simulation
3. Entities with representative dynamics (velocity, acceleration, etc.), signatures, vulnerabilities, equipment, communications, sensors, and weapons
4. CGF uses time invariant information held in CDB such as:
+
a. Terrain altimetry and raster imagery
b. Cultural features
c. Linear (vector) and areal information
d. Sensor signatures
e. Moving Models


===== Weather Simulation

Weather Simulation (WX) involves computing and analyzing the various weather components and models around important areas defined in a simulation, in order to produce realistic real-life scenarios for the sub-systems being affected by weather effects. As such, a weather data server typically handles the weather simulation; this server handles requests for weather-related data such as temperature, 3D winds, turbulence gradients, and complex weather objects such as clouds, frontal systems or storm fronts.

WX uses time invariant information held in data store such as terrain elevation and (potentially) significant features with 3D modeled representations to compute weather and wind patterns.

===== Radar

Typical Radar Simulation Models require modeling of all real-life and man-made effects or objects that can cause significant echo returns from the wavelengths of the simulated Radar RF main beam and side lobes. Additionally, LOS computations are necessary for proper target occultation by the Radar.

The Radar subsystem uses time invariant information held in data store such as:

1. Terrain altimetry and Raster materials
2. Cultural features with either 2D and 3D modeled representations
3. Material properties
4. Land/Coastline/Man-Made features
5. Target shapes (RCS polar diagrams, 3D models)


===== Navigation System

The Navigation System provides the navigation information around the areas and routes as defined in a simulation in order to provide precise NAVAIDs data which will generate well correlated subsystems being part of such simulation scenarios.

As such, the Navigation System Simulation handles navigation aids information requests from other simulator client-devices such as:

1. Tactical Air Navigation (TACAN)
2. Automatic Direction Finder (ADF)
3. VHF Omni Range (VOR)
4. Instrument Landing System (ILS)
5. Microwave Landing System (MLS)
6. Doppler Navigation System (DNS)
7. Global Positioning System (GPS)
8. Inertial Navigation Unit (INU)
9. Non-Directional Beacons (NDB)

In addition to the NAVAIDs, the navigational data include datasets such as:

1. Communications Stations data
2. Airport/Heliport (including SIDs, STARs, Terminal Procedure/Approaches, Gates)
3. Runway/Helipad
4. Waypoints
5. Routes
6. Holding Patterns
7. Airways
8. Airspaces


NAV uses time invariant information held in CDB such as:

1. ICAO code and Airport Identifier
2. NAVAIDs frequency, channel, navigational range, power
3. Declination
4. Magnetic variations
5. Communications Stations data
6. Airport/Heliport
7. Runway/Helipad

==== CDB Data Store and Model naming Guidance

===== Sensor Simulation and Base Materials linkage

Sensor simulation typically requires a simulation of the device itself supplemented by a complete simulation of the synthetic environment over the portion of the electromagnetic spectrum that is relevant to this device.  The former simulation is referred to as the Sensor Simulation Model (SSM) while the latter is called the Sensor Environmental Model (SEM).  Most SEMs in existence today rely heavily on environmental database whose content is designed to match the functionality, fidelity, structure and format requirements of the SEM.  The level of realism possible by the SEM depends heavily on the quality, quantity and completeness of the data available.  This makes the environmental database highly device-specific.

The association of material properties to features in the CDB requires two distinct steps.

1.  The first step consists in establishing a correspondence between all of the Base Materials in the CDB data store and the Base Materials directly supported by the SEM of the client-device.  This is a manual task performed by the SEM specialist(s). The specialist must ensure that his SEM has a corresponding Base Material for each of the CDB Base Materials.  In cases where the SEM is simple, it is possible for two or more CDB Base Materials to point to the same SEM Base Material.  Alternately the SEM specialist may choose to create new SEM Base Materials that correspond more closely to the CDB’s Base Materials.  The result of this process is a SEM look-up.
2.  The second step is typically undertaken during the CDB data store initialization by the client-device running the SEM.  During this initialization phase, the SEM reads the content of the global Base Material Table and the SEM look-up provided by the SEM specialist.  This look-up establishes an indirect link between the Base Materials in the CDB data store and the material properties of the client-device’s SEM Base Materials.  In fact, the indirect link (i.e., the look-up table) can be eliminated if the client device internally builds a Materials Properties Table that uses the CDB material keys directly (as illustrated in Figure 2 11: SEM Base Material Properties Table).



image::images/image8.jpeg[width=524,height=362]

image::images/image9.png[Core Figure 2.11.png,width=528,height=216]

[#img_SEMBaseMaterialPropertiesTable,reftext='Figure 1-8']
*Figure 1-8. SEM Base Material Properties Table*





==== SEM – Materials example

We have a Composite Material consisting of four Base Materials.  For the purpose of this example, we will associate hypothetical keys to these materials:


water (key3 = "BM_WATER-FRESH",  BMT's index 0)

vegetation (key21 = " BM_LAND-LOW_MEADOW",  BMT's index 2)

soil (key7 = " BM_SOIL ",  BMT's index 4)

sand (key4 = " BM_SAND ",  BMT's index 9)


The SEM specialist establishes the following correspondence between the CDB Base Materials and his materials (step 1):


key3 to material 8 ("Lake", SEM list's index 8)

key21 to material 3 ("Uncultivated Land",  SEM list's index 3)

key7 to material 7  ("Soil", SEM list's index 7)

key4 to material 12 ("Sand", SEM list's index 12)


During the CDB initialization process (step 2), a look-up table is built as follows:


BMT’s  index 0 is associated  to SEM list's index 8

BMT’s  index 2 is associated  to SEM list's index 3

BMT’s  index 4 is associated  to SEM list's index 7

BMT’s  index 9 is associated  to SEM list's index 12


===== Geospecific viz Geotypical guidance

In most cases, the decision to invoke a modeled representation of a feature as either geotypical or geospecific is clear.  When it comes to real-world recognizable cultural features, the representation of these features is clearly a geospecific model because it is encountered once in the entire CDB and it is unique in its shape, texture, etc.  At the end of the spectrum, many simulation applications use a generic modeled representation for each feature type and then instance that modeled representation throughout the synthetic environment.  For this case, the choice is clearly geotypical.

There are cases however, where the decision to represent features as either geotypical or geospecific is not as clear-cut.  For instance, a modeler may not be satisfied with a single modeled representation for all the hospital features (FeatureCode-FSC = AL015-006); accordingly, he may wish to model two or more variants of hospitals in the CDB.  While each of these modeled representation may not be real-world specific, they are nonetheless variants of hospitals (say by size or by region or country for example).  Usually, the primary motivation for such variations is one of esthetics and realism; it is not necessarily motivated by the need to accurately reflect real-world features.

In making his decision, the modeler should factor-in the following trade-offs:

1.  _CDB Storage Size:_ The size of the CDB is smaller when the cultural features reference geotypical models rather than geospecific models.  This is due to the fact that the modeled representation of geotypical model is not duplicated within each tile – instead, the model appears once in the GTModel library dataset directory.  Clearly, a geotypical model is the preferred choice if the modeler wishes to assign and re-use the same modeled representation for a given feature type.
2.  _Client-device Memory Footprint:_   By assigning a geotypical model to a feature, the modeler provides a valuable “clue” to the client-device that the feature will be instanced throughout the CDB with the same modeled representation.  As a result, client-device should dedicate physical memory for the storage of the geotypical models for later use.
3.  _GTModel Library Management:_  The CDB’s Feature Data Dictionary (FDD) is based on the DIGEST, DGIWG, SEDRIS and UHRB geomatics standards.  These standards are commonly used for the attribution of source vector data in a broad range of simulation applications.  The CDB Feature Data Dictionary acts much like what an English dictionary is to a collection of novels.  As a result, it is possible to develop a universal GTModel Library which is totally independent of the CDB content (just like a dictionary is independent of books).  This universal GTModel Library can be simply copied into the \CDB\GTModel directory.  The structure of the GTModel Library is organized in accordance to the CDB’s FDD – in other words, the models are indexed using the CDB Feature Code.  The indexing approach greatly simplifies the management of the model library since every model has a pre-established location in the library.
4.  _CDB Generation and Update:_ As mentioned earlier, the size of the CDB is smaller when the cultural features reference geotypical models rather than geospecific models.  This is due to the fact that the modeled representation of geotypical model is not duplicated within each tile – instead, the model appears once in the GTModel library dataset directory.  This reduces the amount of time required by the tools to generate and store the CDB onto the disk storage system.  The second benefit of geotypical models comes in the case where a modeler wishes to change the modeled representation of one or more geotypical features type across the entire CDB.  Changes to the modeled representation of a feature type can easily be performed by simply overwriting the desired model in model library.  From then on, all features of that type now reference the updated model – no other changes to the CBD are required.

Note that since the size of the GTModel library is likely to exceed the client-device’s model memory, the client-device must implement a caching scheme which intelligently discards models or portions of models that are deemed less important, used infrequently or not used at all.  It is up to the client-device to accommodate for the disparity between the size of client-device’s model memory and the size of the GTModel library.  Clearly when the disparity is large, the caching algorithm is solicited more frequently and there is more “trashing” of the cache’s content.  The key to a successful implementation of a caching scheme resides in an approach which discards information not actively or currently used by the client-device.  The CDB standards offers a rich repertoire of attribution information so that client-devices can accomplish this task optimally.  Consequently, the client-devices can smartly discard model data that is not in use (e.g., models and/or, textures) during the course of a simulation.  Note that in more demanding cases, client-devices may have to resort to a greater level of sophistication and determine which levels-of-detail of the model geometry and/or model texture are in use in order to accommodate cache memory constraints.  It is clearly in the modeler’s interest to avoid widespread usage of model variants within the GTModel Library.   In doing so, the modeler overly relies on the client-devices abilities to smartly manage its model cache.  As a result, run-time performance may suffer.

As mentioned earlier, the modeled representation of a geotypical model is not duplicated within each tile – instead, the model appears once in the GTModel library dataset directory. As a result, once the model is loaded into memory, it can be referenced without inducing a paging event to the CDB storage system.  Clearly, the paging requirements associated with geotypical features are negligible.  As a result, paging performance is improved because of the reduced IO requirements on the CDB storage system.

=== Primer: Line-of-Sight (LOS) Algorithms Using MinElevation and MaxElevation Components

_Note: Was A.13 in Volume 2 in original submission_

The purpose of the MinElevation and MaxElevation components is to provide the CDB data store with the necessary data and structure to achieve the required level of determinism in the computation line-of-sight calculations with the terrain.  The values of each component are with respect to mean sea level.  Since both the MinElevation and the MaxElevation values are specified in this standard, any line-of-sight algorithm can rapidly assess an intersection status of the line-of-sight vector with the terrain.

There are three cases to consider:

*CASE 1* *– No intersection:*  If all of the LOS Bounding Boxes are above the MinMax Bounding Boxes, then there is no intersection between the line-of-sight vector and the terrain.  No further testing is required.  (Refer to Figure A-16: Case 1 – No Intersection.)

image::images/image10.jpeg[untitled1,width=575,height=205]
*Figure A-16: Case 1 – No Intersection*

*CASE 2* *– Potential intersection:*  If one or more of the LOS Bounding Boxes overlap with a MinMax Bounding Box, then there is a potential intersection between the line-of-sight vector and the terrain.  This step must be repeated with progressively finer level-of-detail versions of the MinElevation and MaxElevation values until Case 1 or Case 3 is encountered.  If the finest level-of-detail is reached and the LOS result still yields a potential intersection status (Case 2), then the LOS algorithm must perform a LOS intersection with the finest LOD of the Primary Terrain Elevation component using the prescribed CDB meshing convention.  (Refer to Figure A-17: Case 2 – Potential Intersection.)

image::images/image11.jpeg[untitled2,width=578,height=202]
*Figure A-17: Case 2 – Potential Intersection*

*CASE 3 – Intersection:*  If one or more of the LOS Bounding Boxes are below the MinMax Bounding Boxes, then there is an intersection between the line-of-sight vector and the terrain.  No further testing is required to determine whether there is intersection or not.  (Refer to Figure A-18: Case 3 – Guaranteed Intersection.)  However, to determine the intersection point, the LOS algorithm must perform the following additional steps.  If (starting with the LOS point-of-origin) one or more of the LOS Bounding Boxes overlap with a MinMax Bounding Boxes, then there is a potential intersection between the line-of-sight vector and the terrain for that MinMax Bounding Box.  This step must be repeated with progressively finer level-of-detail versions of the MinElevation and MaxElevation values until Case 1 or Case 3 is encountered.  If the finest level-of-detail is reached and the LOS result still yields a potential intersection status (Case 2), then the LOS algorithm must perform a LOS intersection with the finest LOD of the Primary Terrain Elevation component using the prescribed CDB meshing convention.

image::images/image12.jpeg[untitled3,width=580,height=199]
*Figure A-18: Case 3 – Guaranteed Intersection*

=== Gamma Tutorial (Was Annex G, Volume 2)

==== Introduction

There is nominally no gamma correction done to the stored samples of CDB imagery files.  As a result, a gamma of 1/2.2 should be applied to imagery data when viewing it through a (sRGB-calibrated) monitor with gamma of 2.2.  The CDB Standard recommends the sRGB IEC 61966-2 standard when performing the calibration of displays (at DBGF or a simulator).  The sRGB standard provides the necessary guidelines for the handling of gamma, and of color (in a device-independent fashion) under specified viewing conditions.

It would be convenient for graphics programmers if all of the components of an imaging system were linear.  The voltage coming from an electronic camera would be directly proportional to the intensity (power) of light in the scene; the light emitted by a CRT would be directly proportional to its input voltage, and so on.  However, real-world devices do not behave in this way.

Real imaging systems will have several components, and more than one of these can be nonlinear.  If all of the components have transfer characteristics that are power functions, then the transfer function of the entire system is also a power function.  The exponent (gamma) of the whole system's transfer function is just the product of all of the individual exponents (gammas) of the separate stages in the system.  Also, stages that are linear pose no problem, since a power function with an exponent of 1.0 is really a linear function.  So a linear transfer function is just a special case of a power function, with a gamma of 1.0.  Thus, as long as our imaging system contains only stages with linear and power-law transfer functions, we can meaningfully talk about the gamma of the entire system.  This is indeed the case with most real imaging systems.

If the overall gamma of an imaging system is 1.0, its output is linearly proportional to its input.  This means that the ratio between the intensities of any two areas in the reproduced image will be the same as it was in the original scene.  It might seem that this should always be the goal of an imaging system: to accurately reproduce the tones of the original scene.  Alas, that is not the case.

When the reproduced image is to be viewed in “bright surround” conditions, where other white objects nearby in the room have about the same brightness as white in the image, then an overall gamma of 1.0 does indeed give real-looking reproduction of a natural scene.  Photographic prints viewed under room light and computer displays in bright room light are typical “bright surround” viewing conditions.

However, sometimes images are intended to be viewed in “dark surround” conditions, where the room is substantially black except for the image.  This is typical of the way movies and slides (transparencies) are viewed by projection.  Under these circumstances, an accurate reproduction of the original scene results in an image that human viewers judge as “flat” and lacking in contrast.  It turns out that the projected image needs to have a gamma of about 1.5 relative to the original scene for viewers to judge it “natural”.  Thus, slide film is designed to have a gamma of about 1.5, not 1.0.

There is also an intermediate condition called “dim surround”, where the rest of the room is still visible to the viewer, but is noticeably darker than the reproduced image itself.  This is typical of television viewing, at least in the evening, as well as subdued-light computer work areas.  In dim surround conditions, the reproduced image needs to have a gamma of about 1.25 relative to the original scene in order to look natural.

The requirement for boosted contrast (gamma) in dark surround conditions is due to the way the human visual system works, and applies equally well to computer monitors.  Thus, a modeler trying to achieve the maximum realism for the images it displays really needs to know what the room lighting conditions are, and adjust the gamma of the displayed image accordingly.

If asking the user about room lighting conditions is inappropriate or too difficult, it is reasonable to assume that the overall gamma (viewing_gamma as defined below) is somewhere between 1.0 and 1.25.  That's all that most systems that implement gamma correction do.

According to PNG (Portable Network Graphics) Specification Version 1.0, W3C Recommendation 01-October-1996 Appendix, Gamma Tutorial,


(http://www.w3.org/TR/PNG-GammaAppendix[_http://www.w3.org/TR/PNG-GammaAppendix_] ):


“All display systems, almost all photographic film, and many electronic cameras have nonlinear signal-to-light-intensity or intensity-to-signal characteristics.  Fortunately, all of these nonlinear devices have a transfer function that is approximated fairly well by a single type of mathematical function: a power function.  This power function has the general equation


output = input ^ gamma


where ^ denotes exponentiation, and “gamma” (often printed using the Greek letter gamma, thus the name) is simply the exponent of the power function.

By convention, “input” and “output” are both scaled to the range [0..1], with 0 representing black and 1 representing maximum white.  Normalized in this way, the power function is completely described by a single number, the exponent “gamma.”

So, given a particular device, we can measure its output as a function of its input, fit a power function to this measured transfer function, extract the exponent, and call it gamma.  We often say “this device has a gamma of 2.5” as a shorthand for “this device has a power-law response with an exponent of 2.5”.  We can also talk about the gamma of a mathematical transform, or of a lookup table in a frame buffer, so long as the input and output of the thing are related by the power-law expression above.

Real imaging systems will have several components, and more than one of these can be nonlinear.  If all of the components have transfer characteristics that are power functions, then the transfer function of the entire system is also a power function.  The exponent (gamma) of the whole system's transfer function is just the product of all of the individual exponents (gammas) of the separate stages in the system.

Also, stages that are linear pose no problem, since a power function with an exponent of 1.0 is really a linear function.  So a linear transfer function is just a special case of a power function, with a gamma of 1.0.

Thus, as long as our imaging system contains only stages with linear and power-law transfer functions, we can meaningfully talk about the gamma of the entire system.  This is indeed the case with most real imaging systems.”

In an ideal world, sample values would be stored in floating point, there would be lots of precision, and it wouldn't really matter much.  But in reality, we're always trying to store images in as few bits as we can.

If we decide to use samples that are linearly proportional to intensity, and do the gamma correction in the frame buffer LUT, it turns out that we need to use at least 12-16 bits for each of red, green, and blue to have enough precision in intensity.  With any less than that, we will sometimes see “contour bands” or “Mach bands” in the darker areas of the image, where two adjacent sample values are still far enough apart in intensity for the difference to be visible.

However, through an interesting coincidence, the human eye's subjective perception of brightness is related to the physical stimulation of light intensity in a manner that is very much like the power function used for gamma correction.  If we apply gamma correction to measured (or calculated) light intensity before quantizing to an integer for storage in a frame buffer, we can get away with using many fewer bits to store the image.  In fact, 8 bits per color is almost always sufficient to avoid contouring artifacts.  This is because, since gamma correction is so closely related to human perception, we are assigning our 256 available sample codes to intensity values in a manner that approximates how visible those intensity changes are to the eye.  Compared to a linear-sample image, we allocate fewer sample values to brighter parts of the tonal range and more sample values to the darker portions of the tonal range.

Thus, for the same apparent image quality, images using gamma-encoded sample values need only about two-thirds as many bits of storage as images using linear samples.

If we consider a pipeline that involves capturing (or calculating) an image, storing it in an image file, reading the file, and displaying the image on some sort of display screen, there are at least 5 places in the pipeline that could have nonlinear transfer functions.  Let's give each a specific name for their characteristic gamma:



1.  Camera_gamma (γ~_c_~ ):The characteristic of the image sensor.
2.  Encoding_gamma (γ~_e_~ ): The gamma of any transformation performed by the software writing the image file.
3.  Decoding_gamma (γ~_d_~ ): The gamma of any transformation performed by any software reading the image file.
4.  LUT_gamma (γ~_lut_~ ): The gamma of the frame buffer LUT, if present.

 In addition, let's add a few other names:

1.  File_gamma (γ~_f_~): The gamma of the image in the file, relative to the original scene, i.e.
+
γ~_f_~ = γ~_c_~ γ~_e_~
2.  DS_gamma (γ~_DS_~): The gamma of the “display system” downstream of the frame buffer. In this context, the term display system encompasses everything after the frame buffer, that is
+
γ~_DS_~ = γ~_lut_~ γ~_crt_~
3.  Viewing_gamma (γ~_v_~): The overall gamma that we want to obtain to produce pleasing images generally 1.0 to 1.25.

When the file_gamma is not 1.0, we know that some form of gamma correction has been done on the sample values in the file, and we call them “gamma corrected” samples. However, since there can be so many different values of gamma in the image display chain, and some of them are not known at the time the image is written, the samples are not really being “corrected” for a specific display condition. We are really using a power function in the process of encoding an intensity range into a small integer field, and so it is more correct to say “gamma encoded” samples instead of “gamma corrected” samples. The CDB standard does not rely on such gamma encoding in order to achieve smaller integer number representations. Instead, the CDB standard relies on standard compression algorithms to achieve an efficient representation of color imagery. footnote:[The JPEG-2000 standard is based on the _sRGB_ default color space per the IEC 61966-2-1 Standard which calls for a gamma 2.2 under the specified viewing conditions]

When displaying an image file on the simulator, the image-decoding software is responsible for making the overall gamma of the system equal to the desired viewing_gamma, by selecting the decoding_gamma appropriately. If the viewing condition is different from the specification, then the decoding process must compensate. This can be done by modifying the gamma values in equation G-1 below by the appropriate factor. If one does modify the gamma values in equation G-1 below, extreme care must be taken to avoid quantization errors when working with 24 bit images. The display_gamma should be measured (and known) for the display rendering the image (either at the DB generation workstation or the simulator). The correct viewing_gamma depends on lighting conditions, and that will generally have to come from the user. In dimly lit office environments, the generally preferred value for viewing gamma is in the vicinity of 1.125 footnote:[Historically, viewing gammas of 1.5 have been used for viewing projected slides in a dark room and viewing gammas of 1.25 have been used for viewing monitors in a very dim room. This very dim room value of 1.25 has been used extensively in television systems and assumes a ambient luminance level of approximately 15 lux (or 1.4 ft-lb). The current proposal assumes an encoding ambient luminance level of 64 lux (or 5. ft-lb) which is more representative of a dim room in viewing computer generated imagery or a FAA level-D approved flight simulator visual system. Such a system assumes a viewing gamma of 1.125 and is thus consistent with the ITU-R BT.709 standard.]. In many digital video systems, camera_gamma is about 0.5. CRT_gamma is typically 2.2, while encoding_gamma, decoding_gamma, and LUT_gamma are all 1.0. As a result, viewing_gamma ends up being about 1.125. Coincidently, this happens to be the optimal viewing gamma for an ambient luminance level of 64 lux or 5 ft-lbt.



[cols="3,1",]
|==========
a|





ifndef::backend-pdf[]
\[\gamma _c \gamma _d \gamma _{DS} = \gamma _v \]

\[\gamma _c \gamma _d \gamma _{lut} \gamma _{crt} = \gamma _v \]

\[0.511 * 1.0 * 1.0 * 2.2 = 1.125 = \gamma _v \]
endif::[]
ifdef::backend-pdf[]
image::math/g1.png[]
endif::[]

|(eq. G-1)
|==========


In a complex system such as a flight simulator, the system architect must be aware of the gamma at every stage of the system, starting from the source of the imagery (e.g. camera or satellite) right through to the simulator’s display device. His objective is to ensure that product of all gammas match the viewing gamma of the simulator.

Given the above assumptions, and our objective of ensuring that the product of all gammas in the viewing chain equals the viewing gamma, the modeler will end up (subjectively) adjusting images to an equivalent file gamma of 1.25.

The bottom portion of the illustration show the path taken by the CDB imagery as it is ingested first by the real-time publisher, then by the IG, the IG color look-up tables and finally through to the visual display system. In this example, we will assume the following:

1.  The imagery file in the CDB is unmodified (i.e. those produced by the Adobe Photoshop at the DBGF). Note that as a result of viewing gamma of γ~_v_~  = 1.25, the file gamma ended up at γ~_f_~  = 1.25 at the DBGF. As a result, the CDB also has a file gamma of γ~_f_~  = 1.25
2.  The IG performs all of its internal operations in a linear color space (i.e. the IG_gamma is γ~_IG_~ = 1.00)
3.  The simulator visual system produces an average scene brightness of approximately 6 ft-lamberts: under these viewing conditions, the viewing gamma is γ~_v_~ = 1.125.
4.  The measured gamma of the visual display system is γ~_crt_~ = 2.025
5.  The content of the IG’s color look-up tables is adjusted to compensate for the gamma of the visual display system, i.e. it is loaded with γ~_lut_~ = (1/2.025)

Given the above assumptions, and our objective of ensuring that the product of all gammas in the chain equals the viewing gamma of 1.125, the required visual run-time publisher gamma must account for the difference in viewing gamma at the DBGF and at the simulator. As a result, the publisher gamma must be (1.125/1.25).

==== Harmonization of Gamma at DBGF with Gamma of Simulator Visual System

Both the modelers and the visual system architects should be keenly aware of the handling of gamma at the Data Store Generation Facility and at the simulator. Figure G‑1: Typical Handling of Gamma at DBGF and Simulator, illustrates the typical handling of gamma in both of these cases.

The top portion of the illustration shows the path taken by source data as a modeler is viewing it at this workstation via the application software. In this example, we will assume the following:

1.  The DBGF imagery application is Adobe Photoshop. The default color space profile used by Adobe Photoshop (i.e. the *.icm file) is the sRGB Color Space Profile which is defined by the sRGB standard to be a gamma of 2.2, therefore the Photoshop uses a  γ~_lut_~ = (1/2.2)
2.  The DBGF workstation is running Windows (therefore the O/S does not gammatize the imagery before sending it to the display, γ~_lut_~ = 1.25)
3.  The measure gamma of the DBGF workstation monitor is γ~_crt_~ = 2.2
4.  The DBGF workstation is located in a dimly lit room, so the viewing gamma is in effect γ~_v_~ = 1.25


image::images/image36.jpeg[untitled1,width=544,height=364]
*Figure G‑1: Typical Handling of Gamma at DBGF and Simulator*

=== Handling of Color

The default CDB standard color space follows the same convention as the Windows sRGB Color Space Profile. _sRGB_ is the default color space in Windows, based on the IEC 61966-2-1 Standard. A _sRGB_ compliant device does not have to provide a profile or other support for color management to work well.

Nonetheless, whether calibrated or not to the IEC Standard, all variants of RGB are typically close enough that undemanding viewers can get by with simply displaying the data without color correction. By storing calibrated RGB, the CDB standard retains compatibility with existing database tools and software programs that expect RGB data, yet provides enough information for conversion to XYZ in applications that need precise colors. Thus, the CDB standard gets the best of both worlds.

Full compliance to the CDB standard requires adherence to the color space described in this section. However, in virtually all cases, direct use of un-calibrated RGB is sufficient. The builders of Synthetic Environment Databases and the users of Visual Systems should be aware of these color space conventions; significant deviation from the underlying IEC assumptions may yield significant color differences.

The CDB standard encoded RGB color tri-stimulus values assume the following:

1.  Display luminance level: 80 cd/m2
2.  Display white point x = 0.3127, y = 0.3291 (D65)
3.  Display model Offset (R, G and B): 0.055
4.  Display Gun/Phosphor Gamma (R, G, and B): 2.2



[#table_cie,reftext='Table G-1']
*Table G-1. CIE Chromaticity for CDB Reference Primaries & CIE Standard Illuminant*

[cols=",,,,",]
|=======================================
| |*Red* |*Green* |*Blue* |*D65 (white)*
|X |0.6400 |0.3000 |0.1500 |0.3127
|Y |0.3300 |0.6000 |0.0600 |0.3291
|Z |0.0300 |0.1000 |0.7900 |0.3583
|=======================================

According to PNG (Portable Network Graphics) Specification Version 1.0, W3C Recommendation 01-October-1996 Appendix, Color Tutorial,

(http://www.w3.org/TR/PNG-GammaAppendix):

“The color of an object depends not only on the precise spectrum of light emitted or reflected from it, but also on the observer, their species, what else they can see at the same time, even what they have recently looked at. Furthermore, two very different spectra can produce exactly the same color sensation. Color is not an objective property of real-world objects; it is a subjective, biological sensation. However, by making some simplifying assumptions (such as: we are talking about _human_ vision) it is possible to produce a mathematical model of color and thereby obtain good color accuracy.

==== Device-dependent Color

Display the same RGB data on three different monitors, side by side, and you will get a noticeably different color balance on each display. This is because each monitor emits a slightly different shade and intensity of red, green, and blue light. RGB is an example of a device-dependent color model; the color you get depends on the device. This also means that a particular color represented as say RGB 87, 146, 116 on one monitor might have to be specified as RGB 98, 123, 104 on another to produce the _same_ color.

==== Device-independent color

A full physical description of a color would require specifying the exact spectral power distribution of the light source. Fortunately, the human eye and brain are not so sensitive as to require exact reproduction of a spectrum. Mathematical, device-independent color models exist that describe fairly well how a particular color will be seen by humans. The most important device-independent color model, to which all others can be related, was developed by the International Commission on Illumination in 1931 (CIE-1931, in French) and is called “CIE XYZ” or simply “XYZ.”

In XYZ, X is the sum of a weighted power distribution over the whole visible spectrum. So are Y and Z, each with different weights. Thus any arbitrary spectral power distribution is condensed down to just three floating-point numbers. The weights were derived from color matching experiments done on human subjects in the 1920s. CIE XYZ has been an International Standard since 1931, and it has a number of useful properties:

1.  Two colors with the same XYZ values will look the same to humans
2.  Two colors with different XYZ values will not look the same
3.  The Y value represents all the brightness information (luminance)
4.  The XYZ color of any object can be objectively measured

Color models based on XYZ have been used for many years by people who need accurate control of color i.e., lighting engineers for film and TV, paint and dyestuffs manufacturers, and so on. They are thus proven in industrial use. Accurate, device-independent color started to spread from high-end, specialized areas into the mainstream during the late 1980s and early 1990s, and CDB takes notice of that trend.

==== Calibrated, Device-Dependent Color

Traditionally, image file formats have used uncalibrated, device-dependent color. If the precise details of the original display device are known, it becomes possible to convert the device-dependent colors of a particular image to device-independent ones. Making simplifying assumptions, such as working with CRTs (which are much easier than printers), all we need to know are the XYZ values of each primary color and the CRT exponent.

So why does not the CDB standard store images in XYZ instead of RGB? Well, two reasons. First, storing images in XYZ would require more bits of precision, which would make the files bigger. Second, all programs would have to convert the image data before viewing it. But more importantly, whether calibrated or not, all variants of RGB are close enough that undemanding viewers can get by with simply displaying the data without color correction. By storing calibrated RGB, the CDB standard retains compatibility with existing database tools and software programs that expect RGB data, yet provides enough information for conversion to XYZ in applications that need precise colors. Thus, we get the best of both worlds.

=== What are chromaticity and luminance?

Chromaticity is an objective measurement of the color of an object, leaving aside the brightness information. Chromaticity uses two parameters x and y, which are readily calculated from XYZ:


[cols="3,1",]
|==========
a|


ifndef::backend-pdf[]
\[x = X / (X + Y + Z) \]

\[y = Y / (X + Y + Z) \]
endif::[]
ifdef::backend-pdf[]
image::math/g3.png[]
endif::[]

|(eq. G-3)
|==========

XYZ colors having the same chromaticity values will appear to have the same hue but can vary in absolute brightness. Notice that x,y are dimensionless ratios, so they have the same values no matter what units we've used for X,Y,Z.

The Y value of an XYZ color is directly proportional to its absolute brightness and is called the luminance of the color. We can describe a color either by XYZ coordinates or by chromaticity x,y plus luminance Y. The XYZ form has the advantage that it is linearly related to RGB intensities.

=== How are computer monitor colors described?

The “white point” of a display device is the chromaticity x,y of the monitor's nominal white, that is, the color produced when R = G = B = maximum.

It's customary to specify CRT monitor colors by giving the chromaticities of the individual phosphors R, G, and B, plus the white point. The white point allows one to infer the relative brightness of the three phosphors, which isn't determined by their chromaticities alone.

*NOTE:* The absolute brightness of the monitor is not specified. For computer graphics work, we generally don't care very much about absolute brightness levels. Instead of dealing with absolute XYZ values (in which X,Y,Z are expressed in physical units of radiated power, such as candelas per square meter), it is convenient to work in “relative XYZ” units, where the monitor's nominal white is taken to have a luminance (Y) of 1.0. Given this assumption, it's simple to compute XYZ coordinates for the monitor's white, red, green, and blue from their chromaticity values.

=== How do I convert from source_RGB to XYZ

Make a few simplifying assumptions first, like the monitor really is jet black with no input and the guns don't interfere with one another. Then, given that you know the

CIE XYZ values for each of red, green, and blue for a particular monitor, you put them into a matrix M:

[cols="4,1",]
|===========
a|


ifndef::backend-pdf[]
\[ M =
 \begin{bmatrix}
  X_r & X_g & X_b \\
  Y_r & Y_g & Y_b \\
  Z_r & Z_g & Z_b
 \end{bmatrix}
\]
endif::[]
ifdef::backend-pdf[]
image::math/g4.png[]
endif::[]

|(eq. G-4)
|===========




RGB intensity samples normalized to the range zero to one can be converted to XYZ by matrix multiplication.

NOTE: If you the RGB samples are gamma-encoded, the gamma encoding must be un-done.

[cols="4,1",]
|===========
a|


ifndef::backend-pdf[]
\[
\begin{bmatrix}
 X \\
 Y \\
 Z
\end{bmatrix}
 = M
 \begin{bmatrix}
  R \\
  G \\
  B
 \end{bmatrix}
\]
endif::[]
ifdef::backend-pdf[]
image::math/g5.png[]
endif::[]

|(eq. G-5)
|===========


In other words, X = Xr*R + Xg*G + Xb*B, and similarly for Y and Z. You can go the other way too:

[cols="4,1",]
|===========
a|


ifndef::backend-pdf[]
\[
\begin{bmatrix}
 R \\
 G \\
 B
\end{bmatrix}
= M ^{-1}
\begin{bmatrix}
 X \\
 Y \\
 Z
\end{bmatrix}
=
\begin{bmatrix}
  3.2410 & -1.5374 & -0.4986 \\
  -0.9692 & 1.8760 & 0.0416 \\
  0.0556 & -0.2040 & 1.0570
\end{bmatrix}
\begin{bmatrix}
 X \\
 Y \\
 Z
\end{bmatrix}
\]
endif::[]
ifdef::backend-pdf[]
image::math/g6.png[]
endif::[]

|(eq. G-6)
|===========


Where __M__^-1^= The inverse of the matrix _M_ used to go from XYZ-1931 color space to the CDB specification RGB color space.


In the RGB encoding process, negative sRGB tri-stimulus values, and sRGB tri-stimulus values greater than 1,00 are not retained. When encoding software cannot support this extended range, the luminance dynamic range and color gamut of RGB is limited to the tri-stimulus values between 0,0 and 1,0 by simple clipping.

According to PNG (Portable Network Graphics) Specification Version 1.0, W3C Recommendation 01-October-1996 Appendix, Color Tutorial,

(http://www.w3.org/TR/PNG-GammaAppendix):

“The gamut of a device is the subset of visible colors that the device can display. (Note that this has nothing to do with gamma.) The gamut of an RGB device can be visualized as a polyhedron in XYZ space; the vertices correspond to the device's black, blue, red, green, magenta, cyan, yellow, and white.

Different devices have different gamut (e.g. database generation workstation, simulator display systems). In other words one device may be able to display certain colors (usually highly saturated ones) that another device cannot. The gamut of a particular RGB device can be determined from its R, G, and B chromaticities and white point.

Converting image data from one device to another generally results in gamut mismatches colors that cannot be represented exactly on the destination device. The process of making the colors fit, which can range from a simple clip to elaborate nonlinear scaling transformations, is termed gamut mapping. The aim is to produce a reasonable visual representation of the original image.
